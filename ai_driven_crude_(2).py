# -*- coding: utf-8 -*-
"""AI-Driven_Crude (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iuyhlzp9wdDPw7xOfhZU4OsQdXyogIYN

# **AI-Driven Crude: A Comprehensive Literature Review on Artificial Intelligence Techniques for Crude Oil Price Forecasting**

AI-Driven Crude offers a thorough literature review within a Jupyter Notebook, exploring the application of various artificial intelligence techniques for forecasting crude oil prices. Through comprehensive analysis and synthesis of existing research, this project provides valuable insights into the effectiveness and limitations of AI-driven approaches in predicting crude oil price movements, aiding decision-making in energy markets.

We're going to take the following approach:

  1. Problem definition
  2. Data
  3. Evaluation
  4. Features
  5. Modelling
  6. Experimentation

## **1. Problem Statement**
In a statement,
> How can CNN-Long Short-Term Memory (LSTM) models be optimally utilized within the 'AI-Driven Crude' project to enhance the accuracy and reliability of crude oil price forecasting, considering the temporal dependencies and nonlinear patterns inherent in market data?

## **2. Data**
The data is downloaded from kaggle Oil Prices Dataset: https://www.kaggle.com/datasets/mabusalah/brent-oil-prices/data


## **3. Evaluation**
> In assessing the performance of predictive models , evaluation metrics such as loss and mean absolute error (MAE)  provide comprehensive insights into the model's ability to balance precision, recall, and discrimination, which are crucial for effective precision.

## **4. Features**
This is where you'll get different information about each of the features in your data.

**Create data dictionary:**

1. Date
2. Price
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Preparing the tools**

We're going to use pandas, Matplotlib and NumPy for data analysis and manipulation.
"""

!pip install keras

# Commented out IPython magic to ensure Python compatibility.
# Import all the tools we need

# Regular EDA (Exploratory Data Analysis) and plotting libraries
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
# %matplotlib inline

# Import required tools for modelling
import statsmodels.api as sm
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from sklearn.metrics import mean_squared_error
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
import plotly.graph_objects as go
from plotly.subplots import make_subplots

"""## **Load Data**"""

df = pd.read_csv(r"/BrentOilPrices.csv")
df.shape # (rows, columns)

"""## **Data Exploration (exploratory data analysis or EDA)**

The goal here is to find out more about the data and become a subject matter expert on the dataset we're working with.

1. What question(s) are you trying to solve?
2. What kind of data do we have and how do we treat different types?
3. What's missing from the data and how do you deal with it?
4. Where are the outliers and why should you care about them?
5. How can you add, change and remove features to get more out of your data?



Let us list down some points that can be addressed with the analsysis.

1. Temporal Pattern Recognition: Employ LSTM models to capture temporal dependencies and cyclical patterns in historical crude oil price data, enabling more accurate forecasting of future price movements.

2. Feature Engineering: Investigate effective feature engineering techniques to preprocess input data for LSTM models, including lagged variables, technical indicators, and external factors such as geopolitical events or economic indicators.

3. Hyperparameter Optimization: Conduct rigorous hyperparameter tuning experiments to optimize the architecture and parameters of LSTM models, maximizing their predictive performance and robustness in crude oil price forecasting tasks.

4. Model Interpretability: Develop methods for interpreting the predictions made by LSTM models, such as attention mechanisms or sensitivity analysis, to provide insights into the underlying factors driving crude oil price movements.

5. Uncertainty Quantification: Explore techniques for uncertainty quantification in LSTM-based forecasts, including probabilistic forecasting methods or ensemble approaches, to assess the reliability and risk associated with predicted crude oil price trajectories.



"""

df.head()

df.tail()

df.info()

df.describe()

""">Let us check if there are any nul values"""

df.isna().sum()

""">The are no missing values in this dataset `df`.

> Drop any negative values from the dataset
"""

indexNames = df[df['Price'] < 0].index
df.drop(indexNames , inplace=True)
df.shape

"""** Filling date gaps for smooth model training"""

#make dates the index
df.set_index(pd.DatetimeIndex(df.Date), inplace=True)
#fix the Date column
df.Date = df.index.values
df.shape

df['Date'] = pd.to_datetime(df['Date'])
df.head()

"""**Preparing the time series dataset**"""

time_step = [i for i in range(df.shape[0])]
series = np.array(df[['Price']])
time = np.array(time_step)

"""**Visualizing Full Data as a line plot**"""

g = sns.lineplot(x='Date',y='Price',data = df)
plt.title("Brent Oil Price Trend")

def plot_price_trend(df, start_date, end_date):
    mask = (df['Date'] > start_date) & (df['Date'] <= end_date)
    sdf = df.loc[mask]
    plt.figure(figsize = (10,5))
    chart = sns.lineplot(x='Date',y='Price',data = sdf)
#     chart.set_xticklabels(chart.get_xticklabels(), rotation=45)
    plt.title("Brent Oil Price Trend")

plot_price_trend(df,'2017-01-01','2019-01-01')

"""## **Modelling**"""

df.head()

df.tail()

df.isna().sum()
df.shape

"""**Split the data set into training and validation sets**"""

split_time = 11500
time_train = time[:split_time]
x_train = series[:split_time]
x_label_train = df[['Date']][:split_time]

time_valid = time[split_time:]
x_valid = series[split_time:]
x_label_valid = df[['Date']][split_time:]

"""**Prepare functions for model training and model tuning.**"""

def plot_series(time, series, format = "-", start = 0, end = None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel("Time")
    plt.ylabel("Price")
    plt.grid(True)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis = -1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift = 1, drop_remainder = True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

def model_forecast(model, series, window_size):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size, shift = 1, drop_remainder = True)
    ds = ds.flat_map(lambda w: w.batch(window_size))
    ds = ds.batch(32).prefetch(1)
    forecast = model.predict(ds)
    return forecast

"""**Setting Hyperparameters values**"""

window_size = 64
batch_size = 256
shuffle_buffer_size = 1000
epochs = 400

"""**Now let's Tune the model.**"""

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)

train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
valid_set = windowed_dataset(x_valid, window_size, batch_size, shuffle_buffer_size)
print(train_set)
print(x_train.shape)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters = 32, kernel_size = 5,
                      strides = 1, padding = "causal",
                      activation = "relu",
                      input_shape = [None, 1]),
  tf.keras.layers.LSTM(64, return_sequences = True),
  tf.keras.layers.LSTM(64, return_sequences = True),
  tf.keras.layers.Dense(30, activation = "relu"),
  tf.keras.layers.Dense(10, activation = "relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400)
])

lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10 ** (epoch / 20))
optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-8, momentum = 0.9)
model.compile(optimizer = optimizer, loss = tf.keras.losses.Huber(), metrics = ["mae"])
history_tune = model.fit(train_set, epochs=100, callbacks = [lr_schedule])

"""**Now let's plot graph for Loss and Learning Rate**"""

fig = go.Figure()
obj = go.Scatter(x = history_tune.history["learning_rate"], y = history_tune.history["loss"], mode = 'lines', name = 'Loss')
fig.add_trace(obj)
fig.update_xaxes(type="log")
fig.update_layout(title='Loss', xaxis_title='Learning Rate', yaxis_title='Loss',
                  xaxis_tickformat='.1e',)
fig.show()

"""**Now let's check the values for min_loss and learning rate.**"""

min_loss = min(history_tune.history["loss"])
print(f'min_loss: {min_loss}')
index = np.argmin(history_tune.history["loss"])
learning_rate = history_tune.history["learning_rate"][index]
print(f'learning_rate: {learning_rate}')

"""**Now let's train the model**"""

def build_model():
    model = tf.keras.models.Sequential([
      tf.keras.layers.Conv1D(filters = 60, kernel_size = 5,
                          strides = 1, padding = "causal",
                          activation = "relu",
                          input_shape = [None, 1]),
      tf.keras.layers.LSTM(60, return_sequences = True),
      tf.keras.layers.LSTM(60, return_sequences = True),
      tf.keras.layers.Dense(30, activation = "relu"),
      tf.keras.layers.Dense(10, activation = "relu"),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 400)
    ])

    optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate, momentum = 0.9)
    model.compile(loss = tf.keras.losses.Huber(), optimizer = optimizer, metrics = ["mae"])
    return model

model = build_model()
valid_set = windowed_dataset(x_valid, window_size, batch_size, shuffle_buffer_size)
history_train = model.fit(train_set, epochs = epochs, validation_data = valid_set)

"""**Forecasting**"""

rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)
rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]

zoom_begin = epochs - 50
zoon_end = epochs + 1
x_last = list(range(zoom_begin, zoon_end))

fig = make_subplots(rows = 1, cols = 2, start_cell = "top-left")

fig.add_trace(go.Scatter(y = history_train.history["loss"], name='loss'), row = 1, col = 1)

fig.add_trace(go.Scatter(x = x_last, y = history_train.history["loss"][zoom_begin:], name = 'loss last 50 epochs'), row = 1, col = 2)

fig.update_layout(height = 500, title_text = 'Loss ')

fig.show()

"""**Final Training and saving**"""

# The final model will be trained on the entire dataset
full_set = windowed_dataset(series, window_size, batch_size, shuffle_buffer_size)
final_model = build_model()
history_final = final_model.fit(full_set, epochs = epochs)